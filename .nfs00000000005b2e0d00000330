\documentclass[11pt]{article}
\usepackage{booktabs}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{positioning}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{hhline}
\usepackage{latexsym}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{relsize}
%\usepackage{hyperref}
\setlength\titlebox{1.2cm}    % You can expand the title box if you
% really have to
\usepackage{comment}
\usepackage{epstopdf}
\usepackage[small,compact]{titlesec}
\usepackage[font=small,aboveskip=4pt,belowskip=0pt]{caption}
\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage{enumitem}
\setdescription{noitemsep,topsep=0pt}
\captionsetup[algorithm]{labelfont=rm,labelsep=colon,font=footnotesize}\newcommand{\pemp}{\tilde{p}}
%\newcommand{\sto}{\!\shortrightarrow\!}
\newcommand{\sto}{\shortrightarrow}
\newcommand{\argmax}[1]{\ensuremath{\underset{#1}{\mathrm{argmax}\;}}}
\newcommand{\argmin}[1]{\ensuremath{\underset{#1}{\mathrm{argmin}\;}}}
\newcommand*\mystrut[1]{\vrule width0pt height0pt depth#1\relax}
\newcommand{\smathcal}[1]{_{\mathcal{#1}}}

\newcommand{\alert}[1]{{\textcolor{blue}{ALRT: #1}}}

\title{Effective Triangulation for Very Low-Resource Languages}
\author{}

\date{}
\begin{document}
\maketitle

\begin{abstract}
This paper conducts the first in-depth study on the use of triangulation for four real low-resource languages with realistic data settings: Mawukakan and Maninkakan, Haitian Kreyol and Malagasy. We build the first effective system for two of these languages while outperforming the state-of-the-art for Haitian Kreyol. We improve translation quality by injecting more data via pivot languages and show that in realistic data settings carefully considering triangulation design options is important. Furthermore, in all four languages since the low-resource language pair and pivot language pair data typically come from very different domains, we use insights from domain adaptation to fine-tune the weighted mixture of direct and pivot based phrase pairs to significantly improve translation quality.
\end{abstract}

\section{Introduction}
Triangulation for phrase-based statistical machine translation~\cite{Utiyama:07,Cohn:07} refers to the use of a pivot language when translating from a source language with insuffient resources to a target language. Previous research into triangulation for machine translation has assumed the existence of large corpora in the same domain as the low resource language data. However, real-world low resource languages are quite different when it comes to the kind and size of parallel data that is available. This paper considers machine translation into English from four diverse low-resource languages: Mawukakan and Maninkakan, which are West African languages;  Haitian Kreyol, using real-world short messages sent in the aftermath of the Haiti earthquake in 2010; and Malagasy, an Austronesian language that is the national language of Madagascar. This is the first in-depth study of triangulation for all the four languages, and to our knowledge, Mawukakan and Maninkakan have not been studied before in the literature.

Faced with a low-resource language pair, several questions arise when using triangulation: 
\begin{itemize}\addtolength{\itemsep}{-0.4\baselineskip}
	\item How large should the triangulated table be? 
	\item What is the best way to compute feature values in the triangulated phrase table? 
	\item Do we use uniform weights or heuristic values to interpolate the direct and triangulated models?
	\item How does one penalize triangulated phrase pairs with insufficient alignment between the words in the phrase pairs? 
\end{itemize}



\begin{table*}[!ht]
	\footnotesize
	\small
	\centering
	\input{Tables/datasettings.tex}
	\caption{Comparison of various data settings.}
	\label{table:datasettings_differences}
\end{table*}
\subsection{Related Work}
All the papers that use triangulation cite either~\cite{Utiyama:07} or~\cite{Cohn:07}, both published in 2007 (and sometimes cite both of them but use either one model or the other). Subsequent work~\cite{Nakov:12,Nakovemnlp:12,Gispert:06,Huck:12} has also assumed that parallel data in pivot languages can be found in the same domain as the original resource-poor language pair. While~\cite{Utiyama:07} use multi-parallel Europarl comprising 560,000 sentences,~\cite{Cohn:07} use 700,000 Europarl sentences and a low-resource scenario is ``simulated'' by using the top 10K sentences for each language.~\cite{Nakov:12} propose a language-independent approach to improving translation for low-resource languages, but the approach assumes the presence of a resource-rich language that bears similarity to the low-resource language, the similarities helping in creating a large triangulated phrase table. In~\cite{Nakovemnlp:12}, the resource-rich language is adapted to be more like the resource-poor one. Notice that this also assumes both are very similar. Results are reported using both Malay-Indonesian and Bulgarian-Macedonian, the third language being English in both cases.~\cite{Gispert:06} translate Catalan to Spanish via English by using news corpora on both source pivot and pivot target side.~\cite{Huck:12} report on BLEU score improvements by using $10^9$ parallel sentence between German and French. This kind of domain similarity is not easy to find for realistic low-resource settings. Data settings for~\cite{Utiyama:07} and ~\cite{Cohn:07} are shown in Table~\ref{table:datasettings_differences}.

\subsection{Four Very Low-Resource Languages}
	We chose Haitian Kreyol and Malagasy because the datasets represent real-world data (short messages sent after earthquake for Haitian Kreyol; automatically aligned news articles in Malagasy). Moreover, we use the Bible as our source pivot bitext for both as they have no parallel data source with French. This leads to a very disjoint set of systems, a contrast to Multi-parallel Europarl proceedings. On the other hand, Mawukakan and Mawukakan have a very tiny source pivot and source target bitext ($<$6K sentences each), but the source pivot corpus has common sentences with the source target corpus. The corpus sizes are mentioned in Table~\ref{table:datasettings_differences}. 





\begin{comment}
\section{Four real-world low resource languages}
	
	In this paper, we study the effectiveness of pivot-based triangulation for languages with insufficient resources. We report results on using triangulation for Mawukakan, Maninkakan, Malagasy and Haitian-Creole. Mawukakan and Maninkakan are two languages from the Mandekan family, spoken widely in West Africa. The Mandekan languages are a part of the Niger-Congo language family. Maninkakan and Mawukakan have little writing tradition, are written using multiple alphabets \footnote{data we have used has Latin script, obtained via LDC} and have very little resources for Machine Translation (see figure). Malagasy is the national language of Madagascar, spoken by 18 million people worldwide. Haitian-Creole is the national language of Haiti and data used is from the Sixth Workshop on Machine translation, 2011. It comprises short messages sent to the number 4636 after the devastating earthquake in January, 2010. Although nine systems participated in the workshop on Haitian-Creole, the approach of triangulation was not used. To our best knowledge, this is the first in-depth study of triangulation in a real low-resource setting and also the first for the four languages mentioned above. Mawukakan, Maninkakan and Malagasy do not have publicly available SMT systems.

	In the aftermath of the earthquake in Haiti in January, 2010, Mission 4636 set up a service where anyone in Haiti could send a message for free to a phone number 4636\footnote{\emph{http://www.mission4636.org}}. A group of volunteers translated the messages into English and helped the relief organizations provide swift help to the affected masses.  Microsoft Research released a translation system to the public, for Haitian Creole, 5 days after the devastating earthquake~\cite{Lewis:11}. The fast turnaround time\footnote{\emph{To know the exact timeline, refer to http://languagelog.ldc.upenn.edu/nll/?p=2068}} and the usefulness of Machine Translation in the time of crisis inspired the featured task in the 6th Workshop on Statistical Machine Translation. Although Haitian Kreyol is a French-based creole, approach of inducing a Haitian Kreyol to English phrase table by pivoting via French was not used. 

	Malagasy is an Austronesian language and the national language of Madagascar, spoken by 18 million around the world. Although it shares several words with Ma'anyan, it has influences from Arabic, French, Swahili and Bantu. Characters can have diacritics but not always. It follows the latin alphabet but with 21 characters. Finally, the dataset we have is real-world news articles translated by volunteers across the world\footnote{http://www.ark.cs.cmu.edu/global-voices/} and aligned using a sentence aligner. 

	Mawukakan\footnote{http://catalog.ldc.upenn.edu/LDC2005L01} and Maninkakan\footnote{http://catalog.ldc.upenn.edu/LDC2013L01} are two of the four languages of the Mandekan family. They have no writing tradition, are spoken by a few million people around the world and are unique in several ways. 

\end{comment}

\begin{comment}
\section{Triangulation for low-resource languages}
\label{sec:triangulation}

	\begin{table*}
		\small
		\footnotesize
		\centering
		\input{Tables/approaches.tex}
		\caption{Filling the gaps}
		\label{table:filling}
	\end{table*}

	By using two or more translation models, triangulation aims to produce more fluent translations between the source and target languages. Consider a source language \emph{s}, target language \emph{t} and pivot language \emph{p}. Having generated the phrase tables between source pivot, pivot target and source target languages, the process would look like the following :

	\begin{itemize}
		\item  using common pivot phrases, form a triangulated table between source target
		\item  calculate the feature values for the triangulated table
		\item  may or may not need to combine triangulated with the direct 
	\end{itemize}

	By using pivot languages in a multi-parallel corpora, we are effectively taking different paths to the same target translation. It leads to several common pivot phrases which greatly help in inducing a large phrase table with minimal noise. Table~\ref{table:datasettings_differences} shows the difference between two previous papers on using triangulation for multi-parallel corpora compared to the four languages we study in this paper. Observe that all the real-world low resource languages we observe have data in different domains for triangulation. They also have very limited source pivot data, thus, constraining the paths one can take to reach the right target translation. Haitian Kreyol to English corpus has in-domain SMS as 16\% of the corpora and out-of-domain as the rest. The task is to improve the translations on real-world short messages sent in the aftermath of the deadly earthquake in 2010~\cite{WMT:11}. To our best knowledge, Haitian Kreyol (and Malagasy) have no parallel corpora with other languages except English. In a low-resource scenario like this, how does one use triangulation? Moreover, consider Mawukakan and Maninkakan, two of the four languages of the Mandekan family. They have little writing tradition, are spoken by less than 4 million around the world and have a Latin script with frequent diacritics. The corpus we have is less than six thousand parallel sentences between source language \& target (English) and source language \& pivot (French). 

	To enable us to use French as a pivot language for Haitian Kreyol and Malagasy, we use Bible as our source pivot corpora\footnote[1]{http://homepages.inf.ed.ac.uk/s0787820/bible/}. They are aligned by using hunalign~\cite{Hun:05} and comprise 30K parallel sentences. Using Europarl fr-en as our pivot target, we now have a triangulation setting with a different domain for source target (short messages), source pivot (Bible) and pivot target (parlimentary proceedings). Malagasy also does not have parallel corpora with other languages and we use the Bible in a similar manner. For Mawukakan and Maninkakan, our source pivot corpus is extremely tiny and hence, constrains the triangulated table further. In a constrained data scenario, we observe that triangulation only works better for large but not larger sizes of the phrase table (section~\ref{sec:topn}). To penalize the noise, we add two connectivity features~\cite{Ahmed:13} to the log-linear pipeline (section~\ref{sec:strength}). In the absence of lexical alignments, we use a Model 1 score between the phrase pairs (section~\ref{sec:model1}). 

	But, having a triangulated translation model alone will not lead to more fluent translations compared to the baseline, mainly because of the out-of-domain nature of the former. Uniform values~\cite{Cohn:07}  for both the direct and triangulated are counter-intuitive as one is out-of-domain. Our triangulated phrase table should replace out-of-vocabulary words but also improve translations of source phrases already found in the direct system. At the same time, in case of Haitian Kreyol where the domain is significantly different, more trust should be placed on the direct system. For Mawukakan and Maninkakan, the direct system is extremely tiny and the chances of the triangulated table replacing OOVs and significantly improving translations are higher. Thus, heuristically testing out several weights~\cite{Nakov:12} will not lead us to the best possible translations either. In section~\ref{sec:interpolation}, we propose a novel way of using grid search over interpolation co-efficients that leads us to the best interpolated translation model. 



	\input{Tables/Example-each.tex}
\end{comment}

\section{Models}
\label{sec:models}
	

\begin{comment}
\subsection{Fan-out Limit}
\label{sec:topn}
	The size of the triangulated phrase table is controlled by the number of translations \emph{n} considered for a given source phrase. Consider a source phrase \emph{p$_s$} that translates to \emph{p$_p$} in the pivot language. The phrase \emph{p$_p$} has 1293 translations in the pivot target table. Considering all the 1293 translations will result in 1293 translations for the phrase \emph{p$_s$} via one pivot phrase. It is reasonable to expect the phrase \emph{p$_s$} to have multiple pivot translations, all having a high number of translations in pivot target, as the fan-out is quite high. Considering all translations is not recommended for several reasons. Firstly, this will lead to a very large phrase table. Table~\ref{table:allrules} shows the number of rules we can end up with if we consider all possible paths to a target phrase. To put it in perspective, the direct table for Mawukakan and Maninkakan have 51K and 60K phrase pairs respectively. Secondly, alongwith valid translations, triangulation also adds some noise to the translations by considering several translations of the common pivot phrase. We observed that for Mawukakan and Maninkakan, taking the top 100 translations was the same as taking the top 1000 translations, in terms of the BLEU score. We also observed that using more than top 100 translations for Haitian Kreyol and Malagasy led to decline in quality of output translations. Thus, it is important to pay attention to the translation pairs we include in the triangulated phrase table. 

	\begin{table}
		\footnotesize
		\small
		\centering

		\begin{tabular}{p{0.3\linewidth}p{0.3\linewidth}p{0.3\linewidth}}
		\toprule
		Language & Rules \\
		\toprule
		Maninkakan &  106.7M \\
		Mawukakan &  151.6M \\
		\bottomrule
		\end{tabular}
		\caption{Number of rules if all possible paths are considered}
		\label{table:allrules}
	\end{table}
\end{comment}
\subsection{Phrase Scores}
\subsubsection{Product Approach}
	
	An often cited approach to compute feature values is from~\cite{Utiyama:07}, where we multiply the values on the source pivot and pivot target phrase tables, as shown in equations~\eqref{eq:forward} and~\eqref{eq:backward}. 


	\begin{equation} \label{eq:forward}
        p_w(t \mid s) = \sum_{p} p_w(t \mid p) p_w(p \mid s)
	\end{equation}

	\begin{equation} \label{eq:backward}
        p_w(s \mid t) = \sum_{p} p_w(s \mid p) p_w(p \mid t)
	\end{equation}

	We are marginalizing over the pivot phrases (p), essentially making an independence assumption of the following form, as in~\cite{Cohn:07}:  

	\begin{eqnarray*}
		p(t \mid s)&=&\sum_{p}{p(t, p \mid s)}\\
		&=& \sum_{p}{p(t \mid p, s)\,p(p \mid s)}\\
		&\approx& \sum_{p}{p(t \mid p)\,p(p \mid s)}
	\end{eqnarray*}

\subsubsection{Joint and Conditional distributions in triangulation}
\label{sec:joint}
	Another way of calculating the triangulated phrase scores $p_{w}(t \mid s)$ and $p_{w}(s \mid t)$ would be to take the joint probability $p_{w}(s, t)$ and decompose it to get the conditional distributions. But, we do not have the counts in the triangulated phrase table. The pairs that end up in the triangulated table are contingent on the common pivot phrases that connected the source target pair, thus, counting the pairs after triangulation will not be a true reflection of the joint probability. For a triangulated table between \emph{s} and \emph{t}, using source pivot phrase table \emph{sp} and pivot target phrase table \emph{pt}, we can compute the joint probability of a phrase pair (s, t) as follows: 

	\begin{eqnarray*}
		p_{tr}(s, t) &=& \sum_{i}p_{sp}(s, i) p_{pt}(i, t) \\
				&=& \sum_{i}p_{sp}(s \mid i) p_{sp}(i) p_{pt}(i \mid t) p_{pt}(t)
	\end{eqnarray*} 
	

	This is a more accurate description of the joint probability of the (s, t) phrase pair in the triangulated table because we are using source pivot and pivot target counts, both of which have been extracted from the alignments. As we have the counts for the direct system, computing the joint and the conditional distributions is relatively straight-forward. 

\subsection{Lexical Scores}

\subsection{Product Approach}
	Similar to phrase scores, we compute the triangulated lexical scores using the product of the lexical scores of the source pivot and pivot target tables.   

	\begin{equation}
        p_{lex}(t \mid s) = \sum_{p} p_{lex}(t \mid p) p_{lex}(p \mid s)
	\end{equation}

	\begin{equation}
        p_{lex}(s \mid t) = \sum_{p} p_{lex}(s \mid p) p_{lex}(p \mid t)
	\end{equation}

\subsection{IBM Model1 Alignments}
\label{sec:model1}

	An alternative way to compute the lexical score is to use a IBM Model 1~\cite{Brown:1993} (Model 1, henceforth) score between the phrase pairs in the triangulated table. Treating the triangulated phrase table as a parallel corpus, we learn the Model 1 alignment scores in both directions using 5 iterations of the EM algorithm~\cite{Dempster:77}. Given a foreign sentence f = f$_{1}$, . . . ., f$_{m}$, english sentence e = e$_{1}$, . . . , e$_{l}$, the Model 1 score between the sentences is calculated as follows: 

		\begin{equation}
			p(f, a \mid e) = \frac{\epsilon}{(l+1)^m}\mathlarger{\prod\limits_{j=1}^{m}t(f_{j}|e_{a(j)})}
		\end{equation}

	Why use the Model 1 score? Model 1 learns the likelihood of the alignment of the individual words, while also considering the fact that a triangulated table will have less number of source phrases translating into good and some noisy translations. Noisy translations will automatically get a lower Model 1 score, something less likely to happen when using the simpler approach of multiplying the lexical scores. This effect of noisy translations ending up as a viable translation during decoding is also because of the limited source pivot training corpora available. Several translations have been only seen once and the phrase lengths are not very long either (90\% of Mawukakan and Maninkakan phrase table has less than or equal to 3 words).

	\begin{comment}
	The connectivity features in section~\ref{sec:strength} assign a phrase-level score to a given translation pair. The score does not reflect the actual alignments between the word pairs.  A Model 1 score is also used in~\cite{Cohn:07} in the absence of word alignments. They report a BLEU score improvement of 2 points over the standard feature set when using the Model 1 score, but we observe a different pattern altogether across all the four resource-poor languages~\ref{sec:results}. 
	\end{comment}



\subsection{Connectivity Features}
\label{sec:strength}

	
	The phrase pairs in the triangulated phrase table are contingent upon the common pivot phrases. As a result, we can have phrase pairs that map \textbf{``!''} to a target phrase \textbf{``and making the soup thick !''} in Haitian Kreyol to English triangulated phrase table. Owing to the fan-out nature of triangulation, spurious phrase pairs like above get high enough feature values to end up as a translation during decoding. To reward phrase pairs that have more alignment links between and to penalize pairs that don't, we add two connectivity features to the phrase table, as used in ~\cite{Ahmed:13}. 

	For a source phrase \emph{p$_s$}, target phrase \emph{p$_t$}, and with the number of alignment links between them \emph{N}, the strength will be calculated as follows :

	\begin{equation*}
		source_{strength} = \frac{\mathrm{N}}{\mathrm{S}}
	\end{equation*}

	\begin{equation*}
		target_{strength} = \frac{\mathrm{N}}{\mathrm{T}}
	\end{equation*}

	where S is the length of the source phrase \emph{p$_s$} and T is the length of the target phrase \emph{p$_t$}. To compute the connectivity strength feature, the alignments in the source pivot phrase pair are intersected with the pivot target phrase pair. If the resulting alignment has a higher strength, it implies that a majority of the source words do have an alignment with the target. 




	

\subsection{Translation Model Combination}
\label{sec:interpolation}
	For Haitian Kreyol, we are trying to improve translations for real world short messages using common phrases between Bible and parlimentary proceedings. For Malagasy, we are trying to do the same for news articles. To get the best of both worlds, we would want a $\lambda_{d}$ in equation~\eqref{eq:interpolation} which maximizes our BLEU score, where $p_{d}$ represents the direct translation model while $p_{t}$ represents the triangulated translation model. The question that arises is if we can learn $\lambda_{d}$ or use uniform weights~\cite{Cohn:07} or try various heuristic values and choose the best one~\cite{Nakov:12}. 

	\begin{equation} \label{eq:interpolation}
		p_{interp}(s \mid t) = \lambda_{d} p_{d}(s \mid t) + (1 - \lambda_{d}) p_{t}(s \mid t)
	\end{equation}



	\begin{comment}
	When combining two or more translation models, one needs relevant weights assigned to the features from the various models with the aim of maximizing the BLEU score on a heldout set. When the models are from different domains, we want to find the best parameters such that the best translation wins for each given source phrase. For instance, consider the word ``tranblemannt\`e''. It gets translated to \emph{shaking} by our best baseline system. After interpolating our top-20 triangulated translation model, it gets translated to earthquake. Note that the word earthquake is present in the baseline translation model but does not end up as a translation for the source word ``tranblemannt\`e". 
	

	\end{comment}

	

	\begin{algorithm}
		\small
		\caption{Grid Search for Interpolation}
		\label{algo:condor}
		\textbf{Input:} triangulated phrase table p$_{t}$, \\ direct phrase table p$_{d}$, \\
		$\lambda_{d}$, $\lambda_{t} = 1 - \lambda_{d}$, prev$_{bleu}$ = 0, \\
		minimum = $e ^{-2}$ \\
		\textbf{Output:} best$_{\lambda_{d}}$


		\begin{algorithmic}[1]
			\WHILE{$\delta_{bleu}$ $>$ minimum} \label{aline:condition}
			\STATE{interpolate p$_{d}$, p$_{t}$ to give p$_{interp}$} \label{aline:inter}
			\STATE{Run MERT using p$_{interp}$ as translation model} 
			\STATE{find bleu$_{heldout}$} 
			\STATE{$\delta_{bleu}$ = bleu$_{heldout}$ - prev$_{bleu}$}
			\STATE{prev$_{bleu}$ = bleu$_{heldout}$}
			\STATE{Based on $\delta_{bleu}$, find new$\lambda_{d}$} \label{aline:search}
			\ENDWHILE
		\end{algorithmic}
	\end{algorithm}

	Algorithm~\ref{algo:condor} explains the process of using CONDOR~\cite{Condor:05} to find the best interpolation co-efficient for a given direct and triangulated model. Note that the approach can be easily extended to multiple triangulated models or different co-efficients for each feature. Line~\ref{aline:inter} interpolates the two translation models using equation~\eqref{eq:interpolation}. We re-tune the log-linear weights using MERT for the interpolated feature values (on the same tuning data as the baseline) and use the tuned model to find BLEU score on the same heldout set. Based on the difference between the BLEU score obtained and the previous BLEU(line~\ref{aline:search}), CONDOR searches for the new co-efficient in the corresponding direction. The search will culminate when consecutive BLEU scores show a marginal difference~\ref{aline:condition}. As can be seen in the figure ~\ref{fig:condor}, we started with a value of 0.85 for the direct system from Mawu to English. This got a BLEU score of 10.21.  If we had taken a heuristic approach, we would have just taken 0.50 and 0.50, and stopped. Alternatively, we might have tried 0.50, 0.60, 0.70 and 0.80~\cite{Nakov:12}, keeping in mind the different domains. But, neither strategy would have helped us reach our best BLEU score of 10.91, observed with $\lambda_{d}$ = 0.612. If we use uniform weights for both the tables, we get BLEU scores on heldout as shown in Table~\ref{table:all_results}. In three of four cases, we would not have out-performed our baseline. 


	\begin{table*}
	\footnote
	\tiny
	\begin{tabular}{lr}
		\toprule
			Iteration & $\lambda_{d}$ & BLEU \\
			0 & 0.85 & 9.1 \\
			1 & 0.72 & 8.90 \\
			4 & 0.8 & 8.93 \\
			5 & 0.7 & \textbf{9.69} \\
			6 & 0.9 & 8.47 \\
			7 & 0.69 & 8.82 \\
		\bottomrule
	\end{tabular}
	\caption{BLEU scores with various $\lambda_{d}$ values}
	\end{table*}



	\begin{comment}
	\begin{table*}
	\footnote
	\tiny
	\begin{tabular}{lr} 
		\toprule
		$\lambda_{d}$ & BLEU \\
		\toprule
		0.85 & 10.01 \\
		0.95 & 10.21 \\
		0.63 & 10.43 \\
		0.53 & 10.41 \\
		0.612 & 10.91 \\
		\bottomrule
	\end{tabular}
	\caption{BLEU scores with various $\lambda_{d}$ values}
	\end{table*} 
	\end{comment}


	
\begin{comment}
	\begin{table}
		\footnotesize
		\small
		\centering
		\input{Tables/half.tex}
		\caption{BLEU scores on devtest if uniform values assumed for both}
		\label{table:half}
	\end{table}
\end{comment}

	

\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}
	Refer to Table~\ref{table:ddtt} for details about the distribution of data for the four languages. Data for Mawukakan and Maninkakan has been released by LDC, comprising 5076 and 6000 sentences respectively. All Mawukakan sentences have English and French translations while not all maninkakan sentences with english translations have a french translation as well.  In the absence of a standard development, heldout and test set, 40\% of total Mawukakan and 33\% of total Maninkakan data was kept aside for the same. We kept aside a significant portion of the tiny corpora to get a better idea of the different models. For Malagasy, the training and development sets have been used as-is\footnote{http://www.ark.cs.cmu.edu/global-voices/}. As there is no separate heldout data, the top 500 sentences of the test set is used as heldout. All experiments in Haitian Kreyol use the same training, development, heldout and test sets as WMT '11. The training corpus for Haitian Kreyol comprises only 16\% in-domain data, while the development, heldout and test sets comprise only real-world short messages. The development, heldout and test sets for Haitian-Creole have \emph{raw} and \emph{clean} versions. The raw versions are the short messages sent as-is, while the clean versions are the same messages with some words corrected or removed, e.g caf* in raw is cafe in clean version. 

\begin{comment}
\subsection{Pre-processing}

	The English and French side of Mawukakan and Maninkakan parallel data sometimes have forward slashes separating equivalent english and french translations. For both, the feminine form was chosen. For instance, a sentence 
	\begin{verbatim}
		he/she/it goes to school
	\end{verbatim} 
		was replaced by the english sentence \\
	\begin{verbatim}
		she goes to school
	\end{verbatim}

	Text between square brackets was removed. As development, heldout and test sets are not separately released, the last 2000 sentences was used for development, heldout and test together, for both Mawukakan and Maninkakan. The top 1000 was kept aside for development, while 500 each was kept aside for heldout and test. The last 2000 sentences make up 40\% of the total data for Mawukakan and 33\% of the total data for Maninkakan. We kept aside a large percentage for development and testing to get a better idea about the difference between the various models. 

	Both Haitian-Creole and Malagasy are tokenized using the French tokenizer that is part of the Moses toolkit while Mawukakan and Maninkakan are tokenized using the English tokenizer. 
\end{comment}

\subsection{Setup}
	All the experiments have been run using the Moses toolkit~\cite{Koehn:07}, following the standard pipeline. After tokenizing and lowercasing and removing any empty lines, the alignments in both directions are generated using GIZA++~\cite{OchNey:03}, followed by the --grow-diag-final-and heuristic to extract phrases. Weights for the features in the log-linear model are learnt by maximizing the BLEU score~\cite{Papineni:02} on a heldout set using MERT~\cite{Och:03}. All BLEU scores reported are case-insensitive. SRILM~\cite{Stolcke:02} was used to generate the language models. For Haitian-Creole, an interpolated 5-gram language model, using the english side of WMT data and the english side of Europarl, is used. For the other three languages, the language model used is 5-gram Gigaword. We use KenLM~\cite{Ken:11} for LM scoring during decoding. 




	\begin{table}
		\small
		\centering
		\input{Tables/devsets.tex}
		\caption{Training, Development, Heldout and Test sets for all four languages}
		\label{table:ddtt}
	\end{table}

\section{Results}
\label{sec:results}
	\subsection{Baselines}
		The baseline BLEU are reported in Table~\ref{sec:results}. All the BLEU scores are reported on the held-out data. For Haitian Kreyol, our BLEU score is +0.6 BLEU points more than the best system from the 2011 Workshop on Haitian Kreyol. We have another baseline we compare against, and that is the BLEU score we can reach if we assumed uniform weights for both baseline and triangulated translation models.  

	\subsection{Design Choices: Results}

	Despite using a disjoint and out-of-domain Bible as source pivot and Europarl as pivot target, both Haitian-Creole and Malagasy lead to a better BLEU score on using an interpolated model compared to both our baselines. As indicated in the example in Section~\ref{sec:interpolation}, words that were mistranslated earlier get the right translations after pivoting via a large and clean fr-en phrase table. 

	For both Mawukakan and Maninkakan, the BLEU scores show a more significant increase of 2.2 and 1.5 BLEU points respectively for the product interpolation model. As the training and source pivot corpora for both comprises commonly spoken sentences that are not very long, the English side of Europarl effectively augments the limited target side of the training corpus, thus, leading to better translations after interpolation. 

	Intuitively, the two connectivity features should penalize the spurious and less aligned phrases, thus, reducing the noise and rewarding the just translations. But, the effect is not observed in the BLEU scores. Except in the case of Haitian Kreyol where it improves by a small margin, adding the two connectivity features reduces the BLEU score. This could be owing to the fact that the source pivot data is tiny and the intersection of the alignments with the clean Europarl alignments is leading to feature values that do not effectively discriminate between the good and bad translation pairs. For instance, in Mawukakan and Maninkakan, 60\% and 66\% phrase pairs have a source connectivity strength of more than 0.5 while 67\% and 69\% have more than 0.5 in the backward direction.~\newcite{Ahmed:13} found the two features useful translating Persian to Arabic via English, thus, going from a morphologically complex language to another complex language via a simpler language in the form of English. 

	Although Model 1 helps in the case of Malagasy and Haitian-Creole, they do not help in the other two languages. Apart from the fact that the source pivot data for both is much smaller than the Bible, the morphology of the language could have also played a role. The diacritics in Mawukakan and Maninkakan are not always used with the same stress. Sometimes, $a$ is $\acute{a}$ while sometimes it is $\grave{a}$. With Model 1's uniform initilization on a triangulated table with fewer source phrases because of the limited data, Model 1 does not always help us get better lexical scores for a given pair than the multiplied ones. 

	Adding the Joint and decomposed conditionals as features does well for Mawukakan and Maninkakan, leading to the best system for both, while IBM Model 1 lexical scores combined with the product of phrase scores works best for Haitian Kreyol and Malagasy. 

		\begin{table*}
			\small
			\centering
			\input{Tables/all-results.tex}
			\caption{All results for all languages}
			\label{table:all_results}
		\end{table*}


	\begin{figure}
		\small
		\centering
		\includegraphics[width=0.45\textwidth]{Figures/condor_run.eps}
		\caption{Grid search over interpolation co-effs leading to a best BLEU of 10.91 using $\lambda_{d}$ = 0.612}
		\label{fig:condor}
	\end{figure}



	\subsection{Examples}
	\begin{table*}
		\small
		\centering
		\input{Tables/BeforeAfter.tex}
		\caption{Examples of improvements in translations}
		\label{table:examples}
	\end{table*}

	In the Table~\ref{table:examples}, we report examples of improvements in translations. In the first example, we observe that two words that were OOVs for our baseline model have translations after triangulation \& interpolation. In the second and third examples, we observe that triangulation leads to better translations. ``The entrance of the child'' is the only translation for the source phrase \emph{$l\acute{a}$} while after triangulation, we have several translations. In case of ``tranblemannt\`e'', our baseline model has several English translations but none of them mention ``earthquake''. They lead to shaking, tension, of the earthquake, but not the word ``earthquake''. 

	\subsection{Significance Tests}

		We report p-values obtained using multeval~\cite{Clark:11} comparing our best performing against two baselines. The results are reported in Table~\ref{table:significance}. We observe that our best system for Mawukakan and Maninkakan is significantly better than our baseline system, while the interpolated system with uniform weights (no iterative learning) achieves comparable performance to our best system. 

		On the other hand, our best system for Haitian Kreyol and Malagasy is not significant with p<0.05 compared to our baseline. However, it is significantly better than what we would have achieved had we used uniform weights. 
		\begin{table*}
			\footnote
			\tiny
			\input{Tables/Significance.tex}
			\caption{p-values: Our best compared to our baseline, and Uniform is the system with same weight for both baseline \& triangulated. All use the same tuning and heldout set}
			\label{table:significance}
		\end{table*}


\begin{comment}
\section{Related Work}
\label{sec:related_work}

	Previous research in triangulation share several characteristics. A large parallel corpora for source, pivot and target corpora derived from the same domain and languages which share several similarities (e.g vocabulary overlap). While~\cite{Utiyama:07} used multi-parallel europarl comprising 560,000 sentences,~\cite{Cohn:07} used 700,000 multi-parallel europarl sentences, while a low-resource scenario was ``simulated'' by using the top10K sentences for each language.~\cite{Nakov:12} propose a language-independent approach to improving translation for low-resource languages, but the approach assumes the presence of a resource-rich language that bears similarity to the low-resource language, the similarities helping in creating a large triangulated phrase table. In~\cite{Nakovemnlp:12}, the resource-rich language is adapted to be more like the resource-poor one. Notice that this also assumes both are very similar. Results are reported using both Malay-Indonesian and Bulgarian-Macedonian, the third language being English in both cases.~\cite{Gispert:06} translate Catalan to Spanish via English by using news corpora on both source pivot and pivot target side.~\cite{Huck:12} report on BLEU score improvements by using $10^9$ parallel sentence between German and French.


	Consider a source language \emph{s}, pivot language \emph{p} and target language \emph{t}. When using the \emph{cascading} approach, we build two systems, between \emph{s} and \emph{p} and between \emph{p} and \emph{t}. Given a test set in \emph{s}, it is first translated to \emph{p} and those output translations are then translated into the target language \emph{t}. We do not report our results on using cascading for various reasons. Firstly, translating the output of a source pivot system trained and tuned on little data will lead to propogation of errors. Secondly, we will need three development sets, one for each system.  Finally, it has been shown before that cascading does not give the most fluent translations.~\newcite{Utiyama:07} compared pivot-based triangulation with cascading using all of multi-parallel europarl, observing that pivot-based methods outperformed cascading.~\newcite{Bertoldi:08} compare and contrast various alignment heuristics that play a role in phrase extraction and propose a new way of random sampling of training data, reporting results on using triangulation for Chinese to Spanish via English.~\newcite{Wu:09} revisit the two approaches while also reporting results on using a synthetic approach. The datasets used are smaller in size compared to Europarl, released as part of IWSLT 2009, but, the design choices one needs to make when using pivot-based triangulation itself is not touched upon. Also, significant amount of monolingual data is used to expand the training data.
	

	The second approach is the pivot-based approach where a triangulated phrase table is generated between the source and target, by using the common pivot phrases between the source pivot and pivot target tables~\cite{Utiyama:07,Cohn:07}. Thus, the triangulated table largely depends on the number of target phrases in the source pivot table. Having a very small source pivot table leads to a relatively smaller triangulated table owing to the fewer number of common pivot phrases with the pivot target system. Across all our experiments, the pivot target system uses Europarl v7, comprising approximately 2 million parallel sentences. But our source pivot for Maninkakan, Mawukakan, Haitian-Creole and Malagasy has 4k, 5k, 30k and 30k parallel sentences respectively.~\newcite{Utiyama:07} observed that the triangulated table was able to achieve comparable BLEU scores to the direct system for French, German and Spanish. This could be owing to the fact that the data comprised multi-parallel 560K sentences.~\newcite{Cohn:07} observe that multiple pivot languages lead to more fluent translations compared to one pivot language. Multiple pivot language lead to multiple alternative translations, thus, increasing phrase coverage and rewarding the more appropriate translations and reducing out-of-vocabulary words further. They also propose a systematic way of combining the triangulated translation model with the direct model using linear interpolation and log-linear interpolation, although they assume a uniform weight for both the models.~\cite{Wu:09} compare~\cite{Utiyama:07} and~\cite{Cohn:07} but using data from a spoken language task. Moreover, the specific design choices one needs to make in pivot-based triangulation are not discussed. Finally, the data was in the same domain as well.~\cite{Bertoldi:08} propose using monolingual data more effectively and suggest sampling methods to improve the translations with triangulation. 

	To our best knowledge, Malagasy, Maninkakan and Mawukakan have not been studied before in the SMT literature. Although 9 systems participated in the workshop task on Haitian Kreyol, the approach of triangulation via French was not used.
\end{comment}
\section{Conclusion}

In this paper, we answered the questions raised in Section 1 about using triangulation for low-resource languages. We found that making intelligent design choices leads to small but consistent gains. We also observed that iterating over model weights for interpolation works out better than choosing weights heuristically (or uniform weights). This effect is more pronounced when using disjoint corpora for triangulation. 

\bibliographystyle{acl2014}
\bibliography{short}
\end{document}
